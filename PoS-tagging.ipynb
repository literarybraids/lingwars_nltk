{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Etiquetado morfológico (_PoS-tagging_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "En este cuaderno vamos a hacer etiquetado morfológico (o _PoS-tagging_) en inglés. Primero veremos cómo se hace en un solo corpus y después compararemos visualmente los resultados con otros corpus. Veremos otras pequeñas funciones para explorar un corpus. Aprenderemos el concepto de frecuencias normalizadas y por qué son importantes en PLN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "[¿PoS? WTF](http://138.201.254.242:8000/user/leticia/notebooks/PoS-tagging.ipynb#¿PoS?-WTF)\n",
    "\n",
    "[_PoS-tagging_ de un corpus](http://138.201.254.242:8000/user/leticia/notebooks/PoS-tagging.ipynb#PoS-tagging-de-un-corpus)\n",
    "\n",
    "1. [Abrir un corpus y almacenar su contenido](http://138.201.254.242:8000/user/leticia/notebooks/PoS-tagging.ipynb#1.-Abrir-un-corpus-y-almacenar-su-contenido)\n",
    "\n",
    "2. [Tokenizar el texto](http://138.201.254.242:8000/user/leticia/notebooks/PoS-tagging.ipynb#2.-Tokenizar-el-texto)\n",
    "\n",
    "3. [_PoS-tagging_](http://138.201.254.242:8000/user/leticia/notebooks/PoS-tagging.ipynb#3.-PoS-tagging)\n",
    "\n",
    "    [Contar una determinada PoS](http://138.201.254.242:8000/user/leticia/notebooks/PoS-tagging.ipynb#Contar-una-determinada-PoS)\n",
    "\n",
    "[Otras funciones de NLTK que podemos aplicar a un corpus](http://138.201.254.242:8000/user/leticia/notebooks/PoS-tagging.ipynb#Otras-funciones-de-NLTK-que-podemos-aplicar-a-un-corpus)\n",
    "\n",
    "[Comparar PoSes de distintos corpus](http://138.201.254.242:8000/user/leticia/notebooks/PoS-tagging.ipynb#Comparar-PoSes-de-distintos-corpus)\n",
    "\n",
    "- [Comparar formas y lemas totales](http://138.201.254.242:8000/user/leticia/notebooks/PoS-tagging.ipynb#Comparar-formas-y-lemas-totales)\n",
    "\n",
    "- [Comparar formas y lemas normalizados](http://138.201.254.242:8000/user/leticia/notebooks/PoS-tagging.ipynb#Comparar-formas-y-lemas-normalizados)\n",
    "\n",
    "[Ejercicios](http://138.201.254.242:8000/user/leticia/notebooks/PoS-tagging.ipynb#Ejercicios)\n",
    "\n",
    "[Referencias](http://138.201.254.242:8000/user/leticia/notebooks/PoS-tagging.ipynb#Referencias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## ¿PoS? WTF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Antes de nada, ¿qué es eso del etiquetado morfológico? ¿Qué es una PoS?\n",
    "\n",
    "Nos referimos básicamente a los tipos de palabras que hay: nombres, adjetivos, verbos, preposiciones, pronombres, adverbios, conjunciones... Probablemente en el instituto te hayan hablado de ello como \"categorías gramaticales\", pero otra forma de llamarlos es \"partes del discurso\", y eso es exactamente lo que significa _PoS_ en inglés, _part of speech_.\n",
    "\n",
    "PoS-tagging vendría a ser, por tanto, dar a cada palabra de un texto ese \"metadato\" sobre su naturaleza gramatical, normalmente convirtiendo una lista de palabras en una lista de parejas (o _duplas_, como dicen los informáticos) de palabra-PoS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## _PoS-tagging_ de un corpus\n",
    "Si nunca has programado antes, es importante que, antes de seguir, entiendas el concepto de [variable](https://www.tutorialspoint.com/python/python_variable_types.htm) y sepas lo que es un [comentario](https://learnpythonthehardway.org/book/ex2.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1. Abrir un corpus y almacenar su contenido\n",
    "El texto con el que vamos a trabajar se encuentra en el archivo austen-emma.txt en el directorio nltk_data/corpora/gutenberg.\n",
    "\n",
    "Antes de nada, tenemos que extraer todo el texto contenido en el archivo para poder procesarlo con Python.\n",
    "\n",
    "En la variable _f_ vamos a almacenar el archivo de texto, y en la variable _raw_, el contenido de ese archivo.\n",
    "Podemos imprimir los primeros 300 caracteres de _raw_ para comprobar qué hay en esa variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Emma by Jane Austen 1816]\n",
      "\n",
      "VOLUME I\n",
      "\n",
      "CHAPTER I\n",
      "\n",
      "\n",
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
      "and happy disposition, seemed to unite some of the best blessings\n",
      "of existence; and had lived nearly twenty-one years in the world\n",
      "with very little to distress or vex her.\n",
      "\n",
      "She was t\n"
     ]
    }
   ],
   "source": [
    "f = open(\"nltk_data/corpora/gutenberg/austen-emma.txt\", \"r\")\n",
    "raw = f.read()\n",
    "#f.close()\n",
    "\n",
    "print(raw[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2. Tokenizar el texto\n",
    "\n",
    "Llamamos \"tokenizar\" a separar el texto en palabras. En realidad, estamos partiendo en trozos y considerando \"tokens\" todo aquello que esté separado por espacios, es decir, todo lo que no son espacios. ¿Y qué hacemos con esos trozos? Los estamos metiendo en una [lista](https://www.tutorialspoint.com/python/python_lists.htm).\n",
    "\n",
    "¿Por qué hace falta tokenizar el texto antes de hacer el etiquetado? Porque si le pasamos el texto en crudo para que nos devuelva las PoSes, intentará sacar la PoS de cada carácter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "En primer lugar importamos el paquete de NLTK y después los módulos que vamos a utilizar. Si no hacemos este paso no podremos usar las funciones que nos interesan de esos módulos.\n",
    "\n",
    "A continuación solo tenemos que usar la función *word_tokenize* sobre la variable en que habíamos guardado el texto en crudo (_raw_) para que nos devuelva la lista de palabras. Guardamos esa lista en la variable _tokens_.\n",
    "\n",
    "Podemos imprimir los primeros tokens para ver que todo ha salido bien y también podemos ver cuántos hay en total usando la función _len_ (que te devuelve lo larga que es la variable que le pases como argumento)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER', 'I', 'Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', ',', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', ';', 'and', 'had', 'lived', 'nearly', 'twenty-one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her', '.', 'She', 'was']\n",
      "191673\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "tokens = word_tokenize(raw)\n",
    "\n",
    "print(tokens[:60])\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3. _PoS-tagging_\n",
    "La función *nltk.pos_tag* es ya la que nos devuelve la PoS de cada palabra.\n",
    "\n",
    "Fíjate en el valor que ha creado al lado de cada palabra; se deduce enseguida a qué corresponden, pero también puedes consultar la [lista de equivalencias completa](https://stackoverflow.com/a/38264311)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[', 'NNS'), ('Emma', 'NNP'), ('by', 'IN'), ('Jane', 'NNP'), ('Austen', 'NNP'), ('1816', 'CD'), (']', 'NNP'), ('VOLUME', 'NNP'), ('I', 'PRP'), ('CHAPTER', 'VBP'), ('I', 'PRP'), ('Emma', 'NNP'), ('Woodhouse', 'NNP'), (',', ','), ('handsome', 'NN'), (',', ','), ('clever', 'NN'), (',', ','), ('and', 'CC'), ('rich', 'JJ'), (',', ','), ('with', 'IN'), ('a', 'DT'), ('comfortable', 'JJ'), ('home', 'NN'), ('and', 'CC'), ('happy', 'JJ'), ('disposition', 'NN'), (',', ','), ('seemed', 'VBD'), ('to', 'TO'), ('unite', 'VB'), ('some', 'DT'), ('of', 'IN'), ('the', 'DT'), ('best', 'JJS'), ('blessings', 'NNS'), ('of', 'IN'), ('existence', 'NN'), (';', ':'), ('and', 'CC'), ('had', 'VBD'), ('lived', 'VBN'), ('nearly', 'RB'), ('twenty-one', 'CD'), ('years', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('world', 'NN'), ('with', 'IN'), ('very', 'RB'), ('little', 'JJ'), ('to', 'TO'), ('distress', 'VB'), ('or', 'CC'), ('vex', 'VB'), ('her', 'PRP'), ('.', '.'), ('She', 'PRP'), ('was', 'VBD'), ('the', 'DT'), ('youngest', 'JJS'), ('of', 'IN'), ('the', 'DT'), ('two', 'CD'), ('daughters', 'NNS'), ('of', 'IN'), ('a', 'DT'), ('most', 'RBS'), ('affectionate', 'JJ'), (',', ','), ('indulgent', 'JJ'), ('father', 'NN'), (';', ':'), ('and', 'CC'), ('had', 'VBD'), (',', ','), ('in', 'IN'), ('consequence', 'NN'), ('of', 'IN'), ('her', 'PRP$'), ('sister', 'NN'), (\"'s\", 'POS'), ('marriage', 'NN'), (',', ','), ('been', 'VBN'), ('mistress', 'NN'), ('of', 'IN'), ('his', 'PRP$'), ('house', 'NN'), ('from', 'IN'), ('a', 'DT'), ('very', 'RB'), ('early', 'JJ'), ('period', 'NN'), ('.', '.'), ('Her', 'PRP$'), ('mother', 'NN'), ('had', 'VBD'), ('died', 'VBN')]\n"
     ]
    }
   ],
   "source": [
    "tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "print(tagged[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Podemos ver qué PoSes distintas contiene el libro y cuántas son.\n",
    "\n",
    "Para ello creamos un bucle _for_, que sirve para iterar sobre una lista. Por ejemplo ahora le estamos diciendo que para cada elemento que encuentre en la lista _tagged_, elemento que hemos llamado _dupla_ como lo podíamos haber llamado de cualquier otra forma, queremos que tome el segundo valor (en Python se empieza a contar desde el 0) y lo añada a la variable _PoSes_ que hemos creado un par de líneas antes.\n",
    "\n",
    "Después, con la función _set_, estamos eligiendo solo los valores distintos que hay dentro de _PoSes_. Podemos imprimirlo y podemos imprimir su longitud (con _len_) para ver cuáles y cuántas son las PoSes que aparecen en el libro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NNP', 'VBZ', 'PRP', \"''\", 'WDT', 'EX', 'RBR', 'UH', 'PRP$', 'VBG', 'WP', 'JJ', 'DT', 'VBD', 'JJR', '$', 'CD', 'IN', ':', '``', 'POS', 'RBS', 'RB', 'WRB', '(', 'CC', 'NNS', 'VBP', 'MD', 'NNPS', 'FW', 'TO', '.', 'PDT', 'NN', 'JJS', ')', ',', 'RP', 'WP$', 'VB', 'VBN'}\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "PoSes = []\n",
    "\n",
    "for dupla in tagged:\n",
    "    PoSes.append(dupla[1])\n",
    "\n",
    "PoSes = set(PoSes)\n",
    "\n",
    "print(PoSes)\n",
    "print(len(PoSes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Contar una determinada PoS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Saber las PoSes de un corpus puede servir para discriminar por PoSes, es decir, ver cuáles son todos los nombres, verbos, adjetivos, adverbios... Estas cuatro categorías, especialmente, nos van a dar una buena idea del léxico (es decir, el [campo semántico](https://es.wikipedia.org/wiki/Campo_sem%C3%A1ntico)) utilizado en cada corpus que tratemos, porque son las categorías que llevan, en mayor parte, la carga semántica. Compara las oraciones:\n",
    "\n",
    "1. El médico operaba en su quirófano desinfectado.\n",
    "2. El jardinero podaba en su jardín floreciente.\n",
    "\n",
    "En el ámbito médico no esperamos encontrar palabras como _jardinero_, _podar_, _jardín_ o _floreciente_, y en un texto sobre jardinería no encontraríamos los nombres, verbos, adjetivos o adverbios como los de la oración (1). ¡Sin embargo, el resto de las palabras las comparten!\n",
    "\n",
    "Es curioso que esta distinción coincida con la que separa entre lo que los lingüistas llamamos _categorías abiertas_ y _cerradas_. Las categorías abiertas son relativamente fáciles de ampliar: creamos constantemente nuevos nombres y adjetivos, pero el proceso de creación de una nueva conjunción, por ejemplo, es mucho más largo.\n",
    "\n",
    "Vamos a fijarnos, por tanto, en dos categorías con mucha carga semántica, como son los nombres y los verbos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Por cada elemento que hay en nuestra variable _tagged_ vamos a fijarnos en el segundo elemento (lo que indicamos con tupla[1]). Solo nos interesa cuando mide más de un carácter, así que ponemos esa condición con _if_ para seguir avanzando. Si además los dos primeros elementos de la tupla (es decir, los dos primeros caracteres) coinciden con \"NN\", entonces nos sigue interesando. Entonces le ordenamos al programa que en ese caso, el primer elemento de la tupla lo vaya anexando a la variable _nombres_ (que hemos declarado anteriormente como lista).\n",
    "\n",
    "Después, es sencillo imprimir un mensaje jugando con los atributos de esa lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay 32070 nombres en total en Emma , de los cuales 4515 son distintos. Estos son los 50 primeros:\n",
      "['[', 'Emma', 'Jane', 'Austen', ']', 'VOLUME', 'Emma', 'Woodhouse', 'handsome', 'clever', 'home', 'disposition', 'blessings', 'existence', 'years', 'world', 'daughters', 'father', 'consequence', 'sister', 'marriage', 'mistress', 'house', 'period', 'mother', 'remembrance', 'caresses', 'place', 'woman', 'governess', 'mother', 'affection', 'years', 'Miss', 'Taylor', 'Mr.', 'Woodhouse', 'family', 'governess', 'friend', 'fond', 'daughters', 'Emma', 'Between', 'intimacy', 'sisters', 'Miss', 'Taylor', 'office', 'governess']\n"
     ]
    }
   ],
   "source": [
    "nombres = []\n",
    "\n",
    "for tupla in tagged:\n",
    "    if len(tupla[1]) > 1:\n",
    "        if tupla[1][:2] == 'NN':\n",
    "            nombres.append(tupla[0])\n",
    "\n",
    "title = \"Emma\"\n",
    "            \n",
    "print(\"Hay\", len(nombres), \"nombres en total en\", title, \", de los cuales\", len(set(nombres)),\n",
    "      \"son distintos. Estos son los 50 primeros:\")\n",
    "print(nombres[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Y así podemos sacar cualquier PoS, es cuestión de dar con la regla que nos permita cazar cada ejemplar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay 31358 verbos en total en Emma , de los cuales 2885 son distintos. Estos son los 50 primeros:\n",
      "['CHAPTER', 'seemed', 'unite', 'had', 'lived', 'distress', 'vex', 'was', 'had', 'been', 'had', 'died', 'have', 'had', 'been', 'supplied', 'had', 'fallen', 'had', 'been', '_them_', 'was', 'had', 'ceased', 'hold', 'had', 'allowed', 'impose', 'being', 'passed', 'had', 'been', 'living', 'friend', 'attached', 'doing', 'liked', 'esteeming', 'directed', 'were', 'having', 'think', 'were', 'threatened', 'was', 'did', 'came', 'married', 'was', 'brought']\n"
     ]
    }
   ],
   "source": [
    "verbos = []\n",
    "\n",
    "for tupla in tagged:\n",
    "    if len(tupla[1]) > 1:\n",
    "        if tupla[1][:1] == 'V':\n",
    "            verbos.append(tupla[0])\n",
    "\n",
    "print(\"Hay\", len(verbos), \"verbos en total en\", title, \", de los cuales\", len(set(verbos)),\n",
    "      \"son distintos. Estos son los 50 primeros:\")\n",
    "print(verbos[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Otras funciones de NLTK que podemos aplicar a un corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Si importamos Text, podemos convertir el texto tokenizado en un objeto de texto explorable a través de interfaces sencillas. Se puede pintar un gráfico de dispersión para ver cómo está repartida una palabra en un texto, sacar palabras que comparten contextos con la palabra dada, visualizar la cantidad de concordancias que queramos..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "/usr/lib/python3/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEZCAYAAABWwhjiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkqRm/x+X0n6PHay2AwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f36457a1f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "she he you i her there him what harriet that emma this weston them\n",
      "elton they and me herself all\n",
      "Displaying 20 of 1970 matches:\n",
      "r her . The want of Miss Taylor would be felt every hour of every day . She re\n",
      " ; but Emma was aware that great must be the difference between a Mrs. Weston \n",
      "ong October and November evening must be struggled through at Hartfield , befo\n",
      "il , but not one among them who could be accepted in lieu of Miss Taylor for e\n",
      "ther awoke , and made it necessary to be cheerful . His spirits required suppo\n",
      " , my dear . '' `` How often we shall be going to see them , and they coming t\n",
      "d they coming to see us ! -- We shall be always meeting ! _We_ must begin ; we\n",
      "ing . We must go in the carriage , to be sure . '' `` The carriage ! But James\n",
      "; -- and where are the poor horses to be while we are paying our visit ? '' ``\n",
      " paying our visit ? '' `` They are to be put into Mr. Weston 's stable , papa \n",
      "st night . And as for James , you may be very sure he will always like going t\n",
      "d never bangs it . I am sure she will be an excellent servant ; and it will be\n",
      "be an excellent servant ; and it will be a great comfort to poor Miss Taylor t\n",
      "ee his daughter , you know , she will be hearing of us . He will be able to te\n",
      ", she will be hearing of us . He will be able to tell her how we all are . '' \n",
      "r tolerably through the evening , and be attacked by no regrets but her own . \n",
      "are of what sort of joy you must both be feeling , I have been in no hurry wit\n",
      "dependence ! -- At any rate , it must be better to have only one to please tha\n",
      "e to Emma herself , she knew it would be so much less so to her father , that \n",
      "t one . The chances are that she must be a gainer . '' `` Well , '' said Emma \n"
     ]
    }
   ],
   "source": [
    "from nltk import Text\n",
    "\n",
    "tokens = Text(tokens)\n",
    "\n",
    "tokens.dispersion_plot([\"talk\", \"love\", \"friend\"])\n",
    "\n",
    "tokens.similar(\"it\")\n",
    "\n",
    "tokens.concordance(\"be\", lines=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Comparar PoSes de distintos corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Ahora vamos a aplicar todo lo que hemos aprendido hasta ahora en distintos corpus a la vez. (A partir de aquí no vamos a aprender nuevas funcionalidades de NLTK, pero sí de Python; vamos a explorar corpus con los datos que ahora sabemos extraer de ellos.)\n",
    "\n",
    "Para ello nos va a ser muy útil definir funciones. En Python (y en la mayoría de lenguajes de programación) no solo se pueden usar las funciones del sistema o de las librerías que importemos, sino que podemos crear nuestras propias funciones con _def_ e indicando a continuación qué pasos debe seguir esa función. [Más instrucciones sobre funciones](https://www.tutorialspoint.com/python/python_functions.htm).\n",
    "\n",
    "Lo primero que vamos a hacer es una función que aplique los pasos del 1 al 3 que hemos hecho antes con un corpus. Así, solo tendremos que llamarla y pasarle el argumento que apunte al corpus que queramos para que nos devuelva directamente la lista de palabras-PoSes. Por eso la hemos llamado directamente _tag_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def tag(filename):\n",
    "    f = open(filename, \"r\")\n",
    "    raw = f.read()\n",
    "    f.close()\n",
    "    tokens = word_tokenize(raw)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    return tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Después, tiene sentido crear distintas funciones para contar distintas PoSes. De nuevo recuperamos las condiciones que pusimos antes para obtener nombres y verbos. Vamos a hacer que nos devuelva dos valores: el del número total de formas nominales/verbales que aparecen en el texto y el del número total de formas **distintas** que aparecen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def count_nouns(tagged_text):\n",
    "    nouns = []\n",
    "    for tupla in tagged_text:\n",
    "        if len(tupla[1]) > 1:\n",
    "            if tupla[1][:2] == 'NN':\n",
    "                nouns.append(tupla[0])\n",
    "    return len(nouns), len(set(nouns))\n",
    "\n",
    "def count_verbs(tagged_text):\n",
    "    verbs = []\n",
    "    for tupla in tagged_text:\n",
    "        if tupla[1][:1] == 'V':\n",
    "            verbs.append(tupla[0])\n",
    "    return len(verbs), len(set(verbs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Ahora simplemente vamos a hacer una lista manejable con los corpus que vamos a querer comparar. Guardamos en cada variable el sitio en que está el archivo de texto, para solo tener que usarlo una vez, y después creamos la lista con todos ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Emma = \"nltk_data/corpora/gutenberg/austen-emma.txt\" # \"Emma\", de Jane Austen\n",
    "Persuasion = \"nltk_data/corpora/gutenberg/austen-persuasion.txt\" # \"Persuasión\", de Jane Austen\n",
    "Sense = \"nltk_data/corpora/gutenberg/austen-sense.txt\" # \"Sentido y sensibilidad\", de Jane Austen\n",
    "Caesar = \"nltk_data/corpora/gutenberg/shakespeare-caesar.txt\" # \"Julius Caesar\", de Shakespeare\n",
    "Hamlet = \"nltk_data/corpora/gutenberg/shakespeare-hamlet.txt\" # \"Hamlet\", de Shakespeare\n",
    "Macbeth = \"nltk_data/corpora/gutenberg/shakespeare-macbeth.txt\" # \"Macbeth\", de Shakespeare\n",
    "Alice = \"nltk_data/corpora/gutenberg/carroll-alice.txt\" # \"Alicia en el país de las maravillas\", de Lewis Carroll\n",
    "Paradise = \"nltk_data/corpora/gutenberg/milton-paradise.txt\" # \"El paraíso perdido\", de John Milton\n",
    "Leaves = \"nltk_data/corpora/gutenberg/whitman-leaves.txt\" # \"Hojas de hierba\", de Walt Whitman\n",
    "Bible = \"nltk_data/corpora/gutenberg/bible-kjv.txt\" # La Biblia, versión de King James\n",
    "Dorian = \"data/corpus_misc/en/literature/The Picture of Dorian Gray.txt\" # \"El retrato de Dorian Grey\", de Oscar Wilde\n",
    "Ulysses = \"data/corpus_misc/en/literature/Ulysses.txt\" # \"Ulises\", de James Joyce\n",
    "Dracula = \"data/corpus_misc/en/literature/Dracula.txt\" # \"Drácula\", de Bram Stoker\n",
    "Metamorphosis = \"data/corpus_misc/en/literature/Metamorphosis.txt\" # \"Metamorfosis\", de Franz Kafka\n",
    "Friends = \"data/corpus_misc/en/subtitles/Friends/friends.txt\" # Las diez temporadas de la serie de TV \"Friends\"\n",
    "\n",
    "books = [Emma, Persuasion, Sense, Caesar, Hamlet, Macbeth, Alice,\n",
    "         Paradise, Leaves, Bible, Dorian, Ulysses, Dracula, Metamorphosis, Friends]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Finalmente, creamos una función de funciones (¡que acabamos de crear nosotros!). Como le estamos diciendo que nos devuelva dos valores, que a su vez son dobles, lo que nos devuelva serán 4 valores, guardados en una lista de dos duplas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def count(book):\n",
    "    book = tag(book)\n",
    "    nouns = count_nouns(book)\n",
    "    verbs = count_verbs(book)     \n",
    "    return [nouns, verbs]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Vamos a probar si ha funcionado todo esto probando con algunos corpus. Puedes elegir los que quieras de la lista de arriba y sustituirlos para ver cuántos nombres y verbos salen en cada uno. Vemos que, en general, salen más nombres que verbos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(32070, 4515), (31358, 2885)]\n",
      "[(17545, 3106), (16036, 2134)]\n",
      "[(24302, 3527), (22285, 2563)]\n",
      "[(202437, 10226), (126415, 4353)]\n",
      "[(5198, 1498), (4825, 1098)]\n"
     ]
    }
   ],
   "source": [
    "print(count(Emma))\n",
    "print(count(Persuasion))\n",
    "print(count(Sense))\n",
    "print(count(Bible))\n",
    "print(count(Metamorphosis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Comparar formas y lemas totales\n",
    "Ahora tenemos que decidir qué comparar. No tiene sentido comparar apariciones totales de una misma PoS con apariciones únicas de otra; tendremos que comparar valores que representen lo mismo para cada corpus. Por tanto, tenemos que reordenar un poco estas listas que tenemos ahora.\n",
    "\n",
    "Vamos a llamar, por abreviar, _formas_ a las apariciones totales de nombres o verbos y _lemas_ a las apariciones únicas. Pero tengamos en cuenta que no es exactamente así: estaremos contando como lemas distintos cosas como \"woman\" y \"women\", cuando en realidad pertenecen al mismo lema. Pero para ver cómo funciona nos da una aproximación suficientemente ilustrativa (consolémonos con que en inglés no hay muchas formas por lema; el desajuste sería bastante más desastroso en otros idiomas).\n",
    "\n",
    "Nos interesa tener, por un lado, una lista de formas (nominales y verbales) y, por otro, una lista de lemas. Así luego podremos pintar solo las formas y solo los lemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def reordenar(counts):\n",
    "    formas = []\n",
    "    lemas = []\n",
    "    for value in counts:\n",
    "        formas.append(value[0])\n",
    "        lemas.append(value[1])\n",
    "    return formas, lemas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Ahora sacamos la lista de formas y la lista de lemas para cada libro. La siguiente celda tardará unos minutos en ejecutarse cada vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "formas = []\n",
    "lemas = []\n",
    "\n",
    "for book in books:\n",
    "    formas.append(reordenar(count(book))[0])\n",
    "    lemas.append(reordenar(count(book))[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Ahora, ¡a pintar! Para ello vamos a usar la librería [MatPlotLib](https://matplotlib.org/users/pyplot_tutorial.html).\n",
    "\n",
    "Empezamos comparando solo las formas totales. Esperamos que, si hay algún libro de mucha más extensión que los demás, haya una gran diferencia porque, recuerda, estamos contando apariciones totales de nombres y verbos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "booknames = ['Emma', 'Persuasion', 'Sense', 'Caesar', 'Hamlet', 'Macbeth', 'Alice',\n",
    "         'Paradise', 'Leaves', 'Bible', 'Dorian', 'Ulysses', 'Dracula', 'Metamorphosis', 'Friends']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "n = []\n",
    "v = []\n",
    "\n",
    "for forma in formas:\n",
    "    n.append(forma[0])\n",
    "    \n",
    "for forma in formas:\n",
    "    v.append(forma[1])\n",
    "\n",
    "plt.plot(x, n)\n",
    "plt.plot(x, v)\n",
    "plt.axis([0, 16, 0, 225000])\n",
    "plt.title('Nombres y verbos en obras en inglés - formas (sin normalizar)')\n",
    "# You can specify a rotation for the tick labels in degrees or with keywords.\n",
    "plt.xticks(x, booknames, rotation = 'vertical')\n",
    "# Pad margins so that markers don't get clipped by the axes\n",
    "plt.margins(0.1)\n",
    "# Tweak spacing to prevent clipping of tick-labels\n",
    "plt.subplots_adjust(bottom = 0.15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "¡En _Alicia en el país de las maravillas_ se usan en total más formas verbales que nominales! Puedes cambiarle la longitud del eje *y* para comprobarlo. Prueba a ponerlo a 20 000.\n",
    "\n",
    "Como ves, hay mucha disparidad entre obras. La Biblia, el _Ulises_ o las diez temporadas de _Friends_ son muy extensas, por tanto es lógico que contengan más formas que las demás."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Ahora vamos a contar las formas únicas totales. Como no son todas las apariciones, sino que cada vez que aparezca una misma forma se va a contar como 1, esperamos que la gráfica se suavice. Esto nos puede dar una idea de la cantidad de vocabulario nominal y verbal que maneja cada autor con un poco más de rigor: demuestras que conoces una palabra tanto cuando la usas una vez como cuando la usas mil.\n",
    "\n",
    "Pero aún tenemos que pasarle el filtro de normalización para que no tengan ventaja los textos más largos: vas a poder demostrar, con más ventaja frente a otros, que sabes muchos nombres y verbos si te dan más tiempo para hablar que los demás."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "n = []\n",
    "v = []\n",
    "\n",
    "for lema in lemas:\n",
    "    n.append(lema[0])\n",
    "    \n",
    "for lema in lemas:\n",
    "    v.append(lema[1])\n",
    "\n",
    "plt.plot(x, n)\n",
    "plt.plot(x, v)\n",
    "plt.axis([0, 16, 0, 26000])\n",
    "plt.title('Nombres y verbos en obras en inglés - lemas (sin normalizar)')\n",
    "# You can specify a rotation for the tick labels in degrees or with keywords.\n",
    "plt.xticks(x, booknames, rotation = 'vertical')\n",
    "# Pad margins so that markers don't get clipped by the axes\n",
    "plt.margins(0.1)\n",
    "# Tweak spacing to prevent clipping of tick-labels\n",
    "plt.subplots_adjust(bottom = 0.15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Comparar formas y lemas normalizados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Como ya hemos dicho, en la línea de comparar datos lo más equilibrados posible, no tiene mucho sentido comparar formas ni lemas totales de un corpus de 1000 palabras con los de uno de 20 000. Para analizar la variedad de léxico que hay en una obra u otra, además, no sería justo.\n",
    "\n",
    "Así que una cosa que podemos hacer es dividir estos datos con los que hemos estado trabajando hasta ahora entre el número de palabras totales de su respectivo corpus. Es decir, la longitud del objeto que contiene el texto. Vamos a definir una función exactamente igual a la que antes definimos como _count_, solo que esta vez dividiendo los datos entre el número total de palabras de su respectivo corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def normalised_count(book):\n",
    "    book = tag(book)\n",
    "    n_nouns = [count_nouns(book)[0]/len(book), count_nouns(book)[1]/len(book)]\n",
    "    n_verbs = [count_verbs(book)[0]/len(book), count_verbs(book)[1]/len(book)]\n",
    "    return [n_nouns, n_verbs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(normalised_count(Emma))\n",
    "print(normalised_count(Persuasion))\n",
    "print(normalised_count(Sense))\n",
    "print(normalised_count(Bible))\n",
    "print(normalised_count(Paradise))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Volvemos a reordenar como antes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "n_formas = []\n",
    "n_lemas = []\n",
    "\n",
    "for book in books:\n",
    "    n_formas.append(reordenar(normalised_count(book))[0])\n",
    "    n_lemas.append(reordenar(normalised_count(book))[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "n = []\n",
    "v = []\n",
    "\n",
    "for n_forma in n_formas:\n",
    "    n.append(n_forma[0])\n",
    "    \n",
    "for n_forma in n_formas:\n",
    "    v.append(n_forma[1])\n",
    "\n",
    "plt.plot(x, n)\n",
    "plt.plot(x, v)\n",
    "plt.axis([0, 16, 0.1, 0.31])\n",
    "plt.title('Nombres y verbos en obras en inglés - formas (valores normalizados)')\n",
    "# You can specify a rotation for the tick labels in degrees or with keywords.\n",
    "plt.xticks(x, booknames, rotation = 'vertical')\n",
    "# Pad margins so that markers don't get clipped by the axes\n",
    "plt.margins(0.1)\n",
    "# Tweak spacing to prevent clipping of tick-labels\n",
    "plt.subplots_adjust(bottom = 0.15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Esto ya es otra cosa: aquí podemos ver la proporción de formas totales de nombres y verbos en una comparación justa entre corpus.\n",
    "\n",
    "Se me ocurre que en _Alicia en el país de las maravillas_ y en _Friends_ se usan más verbos que nombres porque hay mucho diálogo. Sí, Shakespeare también es diálogo, pero al fin y al cabo es teatro y está más cerca de la poesía. Los libros de un mismo autor, incluso los de un mismo género, comparten muchos rasgos (y por eso hemos escogido estas obras y no otras): las obras de Jane Austen, _Alicia_, _El retrato de Dorian Gray_, _Dracula_, _La metamorfosis_... son todo novelas y andan a la par en el gráfico (entre 0,15 y 0,20 y sin mucha distancia entre formas nominales y verbales). ¿Qué conclusiones sacas tú?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Ahora hacemos lo mismo, pero con los \"lemas\". Ahora sí podemos esperar que la gráfica nos muestre la riqueza léxica de cada obra (al menos al nivel al que nos estamos acercando a ello) y veamos qué autor se lució más en sus obras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "n = []\n",
    "v = []\n",
    "\n",
    "for n_lema in n_lemas:\n",
    "    n.append(n_lema[0])\n",
    "    \n",
    "for n_lema in n_lemas:\n",
    "    v.append(n_lema[1])\n",
    "\n",
    "plt.plot(x, n, label = 'Nombres')\n",
    "plt.plot(x, v, label = 'Verbos')\n",
    "plt.axis([0, 16, 0, 0.13])\n",
    "plt.title('Nombres y verbos en obras en inglés - lemas (valores normalizados)')\n",
    "# You can specify a rotation for the tick labels in degrees or with keywords.\n",
    "plt.xticks(x, booknames, rotation = 'vertical')\n",
    "# Pad margins so that markers don't get clipped by the axes\n",
    "plt.margins(0.1)\n",
    "# Tweak spacing to prevent clipping of tick-labels\n",
    "plt.subplots_adjust(bottom = 0.15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "[¡Baia, baia!](https://img.buzzfeed.com/buzzfeed-static/static/2016-03/17/13/campaign_images/webdr11/14-situaciones-perfectas-para-decir-baia-baia-2-17132-1458237439-0_dblbig.jpg) Parece que a la hora de la verdad la Biblia es la que menos riqueza léxica tiene (esto no es necesariamente malo, solo significa que es más monotemática que las demás, lo cual es cierto). _Friends_ le va a la zaga (estos chicos de Nueva York solo hablan de café y de citas románticas) y se erige como campeón en cuanto a riqueza léxica Shakespeare en _Macbeth_ y en _Hamlet_. De nuevo, observamos cómo la poesía va por su lado y la novela por otro..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Ejercicios\n",
    "- ¿Te atreves a añadir otra PoS a los gráficos? Otra categoría que debería poder dar mucho juego son los adjetivos ;)\n",
    "- Pinta gráficos de dispersión de distintos corpus con las mismas palabras\n",
    "- Saca palabras similares en contexto de una misma palabra en distintos corpus\n",
    "- Encuentra concordancias de una misma palabra en distintos corpus\n",
    "- Y sobre todo, ¡piensa qué puede explicar que salgan resultados distintos!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Referencias\n",
    "Muchas ideas para este cuaderno han salido del [maravilloso blog del NLP Workgroup de Michael Hemenway](http://nlpworkgroup.postach.io/).\n",
    "\n",
    "Para hacer _PoS-tagging_ en español está el [genial cuaderno de _PoS-tagging_ de Víctor Peinado](https://github.com/vitojph/kschool-nlp/blob/master/notebooks-py3/pos-tagger-es.ipynb).\n",
    "\n",
    "Tal vez también te interese leer el [capítulo sobre categorización y etiquetado de palabras del libro de NLTK](http://www.nltk.org/book/ch05.html).\n",
    "\n",
    "En general, hay muchos cuadernos por la red sobre cualquier tema, en [esta recopilación](https://github.com/jupyter/jupyter/wiki/A-gallery-of-interesting-Jupyter-Notebooks) hay sobre _text mining_ y PLN."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
